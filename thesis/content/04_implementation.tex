\chapter{Implementation}
This chapter deals with the implementation details of the campus app. It starts by describing the process for the collection and processing of geodata and explains how the campus map and the navigation system are derived from it. It further showcases the procedure for gathering campus-relevant data from publicly available web sources and concludes with a description of the implementation process for the user interface, which finally merges the campus map, navigation and information layers.

\label{cha:implementation}
\section{Collection of geodata and POI}
Geodata refers to all data about geographic information. In the case of this thesis, it mainly consists of coordinate points, which describe certain campus-relevant entities, such as outlines of buildings, the street network, pathways, entrances as well as boundaries of green areas and water. Except for single-point usage, multiple coordinate points can be grouped into polygons, representing the closed outline of an entity and polylines, mainly used to describe lines, streets and paths. In addition to its coordinates, geodata also describes other attributes of an entity e.g., its name, altitude, width or height. This can be used for tasks like geocoding/reverse geocoding, display of information, 3d terrain generation or general data analysis.

\subsection{Overview of needed geodata}
Geodata is the building block for map generation and routing through a street network. The following list presents an overview of the geodata needed for this implementation:

\begin{itemize}
    \item Buildings: The outlines of buildings are polygons consisting of multiple coordinate points. Additionally to that, the height of each building is needed for appropriate 3D representation.
    \item Streets: Streets are represented by polylines and are (based on their importance, size and purpose) divided into three categories, namely main roads, small roads and pathways. This separation allows the different road types to be designed and displayed independently from each other. They can therefore vary in size, style and coloring when used in the campus map.
    \item Green areas: The outlines of green areas are retrieved in the polygon format.
    \item Water: The outlines of water (especially rivers) are also retrieved in the polygon format.
    \item Entrances to TU Berlin's buildings: To successfully connect TU Berlin's buildings to its underlying street network, entrances need to be defined. A single-point representation paired with an identifier for the buildings is used.
\end{itemize}

\subsection{Collection data from OSM via Overpass Turbo API}
OpenStreetMap (OSM) is a free mapping service, that allows its users to access, edit and download its available geodata. Its main querying language is Overpass Turbo which comes with a web interface for data mining and a respective web API \cite{openstreetmap_overpass_turbo}. This querying system is used to provide a starting point for the complete set of geodata needed for this thesis. The used bounding box for retrieval is (52.49993096650543, 13.307022255971093) for the south-west and (52.52269751545147, 13.341918533901309) for the north-east corner. The following table presents the queried data over all categories together with the node count (one node usually consists of a coordinate point), the type of retrieved data and the respective query.

\begin{table}[!ht]
	\small
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		Entities                & Type of geodata       & Number of queried nodes       & Overpass Turbo query \\
		\hline
        Buildings               & Polygons              & 1949                          & \ref{buildings} \\
		\hline
		Main roads              & Polylines             & 2036                          & \ref{main_roads} \\
		\hline
		Small roads             & Polylines             & 2578                          & \ref{small_roads} \\
		\hline
		Pathways and sidewalks  & Polylines             & 11757                         & \ref{pathways} \\
		\hline
        Green areas             & Polygons              & 5057                          & \ref{green_areas} \\
		\hline
        Water                   & Polygons              & 1651                          & \ref{water} \\
		\hline
        Entrances               & Single points         & 1758                          & \ref{entrances} \\
		\hline
	\end{tabular}
	\caption{Overview of queried geodata}
\end{table}

This results in a total of 26786 queried nodes. Each category is further exported as a separate geoJSON file, containing the relations between individual points (polygon, polyline, single node) as well as other OSM attributes (e.g., the names of buildings and streets, the height of buildings, etc.).

\subsection{Correction of OSM data for map generation in QGIS}
Since the queried data contains several imperfections that are not desirable for map generation, all geoJSON files are imported into QGIS for cleanup. QGIS is an open-source geographic information system, that provides a convenient visual user interface for editing geodata. The following images present a visual reference for the changes performed to the data (note, that all street data is represented in one image for simplicity and that water data does not need cleanup and is therefore not present).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{images/preparing_buildings.png}\\
	\caption{Correction of TU Berlin's building data}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{images/preparing_streets.png}\\
	\caption{Correction of TU Berlin's street network data}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{images/preparing_green_areas.png}\\
	\caption{Correction of TU Berlin's green area data}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{images/preparing_entrances.png}\\
	\caption{Correction of TU Berlin's entrance data}
\end{figure}

The main problem that gets corrected consists of wrong geometry types in the data. Since is desirable that every dataset only contains one kind of geometry (e.g., the data for the buildings should only contain polygon outlines, the street data should only contain polylines, etc.), all wrong geometry types in a dataset are either deleted (e.g., single points in the street or building data) or reshaped to match the expected geometry type (e.g., polylines representing the outline of a building/green area are converted into polygons).

Missing entities (e.g., certain entrances to buildings) are furthermore added and information not related to the campus (mainly data from the street network) is removed.

\section{Generation of digital campus map}
This section provides a detailed overview of the procedure for generating the digital campus map in the Unity3d real-time rendering engine. It starts by describing how the already collected data is used and imported into Unity and further explains how 3d entities are created from it. The section concludes with a brief overview of the map design and the implementation details for common map gestures such as pinch-to-zoom or slide-to-move.

\subsection{Data import and conversion in Unity}
The first step in the map generation process is the import of the manually corrected OSM data from QGIS into Unity3d. This is done in geoJSON format \cite{geoJSON}, which specifies a certain JSON structure for geodata and contains all coordinate points and their relations to each other as well as additional attributes.

After import, a C\# class named GeoNode is introduced, which represents a single node in the OSM data and contains its position. Furthermore, a class for every category of entities on the campus map (buildings, green areas, streets, ...) is implemented, which contains an array of position vectors, derived from the GeoNodes, describing the respective (out)line of its entity. Lastly, methods for parsing geoJSON into their C\# object representations are created.

One main problem during the parsing process is the fact that OSM and the campus map are constructed differently: While OpenStreetMap consists of a map projected onto a sphere, the campus map is generated on a 2-dimensional plane. This results in the usage of different coordinate systems. OpenStreetMap in this case uses the WGS 84/Pseudo-Mercator system \cite{pseudo_mercator_1} \cite{pseudo_mercator_2}, which describes points on Earth as pairs of latitude and longitude. This is ideal for spherical map projection but is not suitable for a 2d plane base, where general length units (e.g., meters) are advantageous. A conversion from WGS 84/Pseudo-Mercator to 2d position vectors (x, y) in meters from the map origin is therefore chosen and integrated into the parsing process. It is further defined that x represents the longitude and y is the original latitude component of WGS 84.

The unit conversion process is based on OpenStreetMap's reference implementation of the WGS 84/Pseudo-Mercator projection \cite{mercator_projection_implementation} in C and computes the following formulas:

\[x(lon) = lon * r\]
\[y(lat) = \ln(\tan(\frac{lat}{2} + \frac{\pi}{4})) * r\]

Where the WGS 84/Pseudo-Mercator coordinates (lat, lon) in radians are converted to (x, y) in approximate meters, with r being the radius of the Earth (note that the WGS 84 coordinates are usually defined in degrees and need to be converted into radians beforehand).

The resulting coordinates are furthermore stored inside the GeoNode class as floats and are also converted into Unity's 3-dimensional Vector3 position system, where a position (x, y) is mapped to Vector3(x, 0, y). To center the campus section of TU Berlin around Unity's origin at Vector3(0, 0, 0), the central map point of campus Charlottenburg (52.5126624, 13.3231489) is subtracted from all OSM data points before conversion.

\subsection{Mesh generation for streets, green areas, water and 3d buildings}
Meshes are the building blocks for every 3-dimensional object rendered on a digital screen. They define the structure of an object and provide parameters for lighting calculations and material mapping and are generated by defining a set of attributes, the most relevant ones being:

\begin{enumerate}
	\item Vertices: Vertices are points in space, that define the corners of an object.
	\item Triangles: To generate a surface area, 3 different previously defined vertices are connected to form a triangle. A net of triangles finally determines the final shape and surface of an object. One important concept to keep in mind is the backface culling step of the render pipeline: For optimization purposes, the rendering engine decides whether surfaces created by triangles are drawn or not by using the order of vertices that define a certain triangle. Unity3d only renders triangles with a clockwise ordering of vertices when projected onto the screen. The whole process of generating a triangle net to fill a predefined surface area is called surface triangulation.
	\item Surface normals: Surface normals are vectors that are perpendicular to their related surface. They are used to differentiate between the "front" and "back" sides of their respective surface and are an important component for calculating the lighting of an object. Generally, each triangle is assigned a surface normal, which can be calculated by taking the cross product of two different edges. (Note: Unity differs from that and uses a surface normal per vertex approach. This normal can be calculated by averaging all normals of triangles containing the respective vertex. For better understanding, the normal per triangle model is nevertheless displayed in the figures.)
	\item UV coordinates: UVs are 2-dimensional coordinates that specify the texture mapping of an object. There are as many UV coordinates as vertices in a mesh. Based on the shape of an object, the UV coordinate system can be mapped differently onto the vertices.
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/mesh_generation_process.png}\\
	\caption{Mesh generation process for a cube}
\end{figure}

In the case of the campus map, several simplifications can be made: Surface normals can be calculated from the predefined vertices and triangles. Since the entities on the campus are only unicolored (no distortable textures are applied) the 2-dimensional UV coordinates can be chosen arbitrarily (or not at all) for every vertex. This breaks down the mesh generation process for the campus map into two separate tasks: Defining the outlining vertices of an entity from its GeoNodes and triangulating its corresponding surface.

Mesh generation for streets: To define the outline of a street, the corresponding GeoNodes, which only represent a polyline, need to be replaced by two separate polylines defining the edges of the street. The width $w$ of the street is then defined by the distance between both outlining polylines. To generate them, every GeoNode at index $i$ with coordinate $\vec{g_{i}}$ of the original polyline can be shifted with a vector $\vec{v}$, which is perpendicular to the road direction at $\vec{g_{i}}$ and the up-facing vector (in the case of Unity Vector3(0, 1, 0)). A pair of outlining points can be therefore generated for every GeoNode coordinate $\vec{g_{i}}$:

\[\vec{eRight_{i}} = (\vec{g_{i + 1}} - \vec{g_{i}}) \times (0, 1, 0) * \frac{w}{2}\]

\[\vec{eLeft_{i}} = (\vec{g_{i + 1}} - \vec{g_{i}}) \times (0, 1, 0) * - \frac{w}{2}\]

Where $\vec{eRight_{i}}$ is the point on the right polyline and $\vec{eLeft_{i}}$ is the point on the left polyline corresponding to $\vec{g_{i}}$ (when looked into the direction $\vec{g_{i + 1}} - \vec{g_{i}}$).

These vertices can be further triangulated by grouping 4 points. To fill the street surface between coordinates $\vec{g_{i}}$ and $\vec{g_{i + 1}}$, the points $A = \vec{eLeft_{i}}$, $B = \vec{eRight_{i}}$, $C = \vec{eRight_{i + 1}}$ and $D = \vec{eLeft_{i + 1}}$ are selected and two triangles $ACB$ and $ADC$ are created.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/street_mesh_generation.png}\\
	\caption{Mesh generation process for a street}
\end{figure}

Mesh generation for buildings: Buildings consist of a ceiling tile and a set of surrounding walls. To determine the outlining vertices, the predefined set of GeoNode coordinates can be extended by shifting it upwards. The amount of shift is determined by the height $h$ of the respective building. For every GeoNode coordinate $\vec{g_{i}}$, a new coordinate $\vec{c_{i}}$ is therefore created and added to the final set of building vertices:

\[\vec{c_{i}} = \vec{g_{i}} + (0, 1, 0) * h\]

The triangulation process for the resulting set of vertices can be broken down into two separate procedures for the ceiling and the walls. On the one hand, a routine similar to the street triangulation process can be applied to create the walls. A single wall is here defined by two consecutive GeoNode coordinates $\vec{g_{i}}$ and $\vec{g_{i + 1}}$ and their corresponding shifted coordinates $\vec{c_{i}}$ and $\vec{c_{i + 1}}$. The surface area between these four points is then exactly triangulated like a piece of street defined by four points (see the previous section).

On the other hand is the triangulation process for the ceiling tile. In this case, a standard algorithm for the triangulation of concave polygons can be applied. The "Triangulation by Ear Clipping" \cite{triangulation_by_ear_clipping} algorithm is used with a C\# implementation for Unity derived from \cite{triangulation_unity_library}. To avoid backface culling problems and for simplicity, all triangles are generated double-sided, with vertices specified in both orderings.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/building_mesh_generation.png}\\
	\caption{Mesh generation process for a building}
\end{figure}

Mesh generation for green areas and water: Since green areas and water both consist of flat and concave polygons, the same procedure can be used for generating both respective meshes. The outlining vertices are in this case simply the predefined GeoNode coordinates and can be triangulated in the same manner as the ceiling of a building.

\subsection{Camera setup}
One main component for the 3d rendering of the campus map is a virtual camera, that can be regarded as a viewport through which the user looks at the scene. Its transform (the set of values that determines the camera's position, rotation and scale) as well as several other parameters specifying how the scene is rendered can be manipulated to achieve a specific look of the scene.

To easily control the camera for movement, zoom and rotation, an empty and transparent object with position $\overrightarrow{p}$ is placed on the ground of the map. This focus point object is furthermore declared as the parent of the camera, which results in the fact that all camera transformations are now relative to the focus point's transform. This means, that $\overrightarrow{p}$ now serves as the camera's world origin and that the camera furthermore has two different transforms: The global transform GC, which reflects the actual position, rotation and scale of the camera in the scene and the local transform LC, which consists of the camera's position, rotation and scale relative to its parental focus point. To make it more comprehensible one can look at an example regarding the positioning of the camera: If the local position of the camera $\overrightarrow{lc}$ is at $(0, 0, 0)$ (and neither a rotation nor a scaling is anywhere applied), the camera's true global position in the scene $\overrightarrow{gc}$ is the focus point's position $\overrightarrow{p}$. In general (without rotation or scaling) $\overrightarrow{gc} = \overrightarrow{p} + \overrightarrow{lc}$, which means that the camera now "follows" the focus point object with offset $\overrightarrow{lc}$ whenever its position changes.

The camera is now locally transformed such that the focus point object always lies in the center of its viewport. In principle, this can be done arbitrarily, but to mimic the perspective of TU Berlin's official campus map \cite{campus_plan}, a setup is chosen where the camera is rotated 45 degrees down (x-axis) and $\overrightarrow{lc} = (0, z, -z), z \geq 0$ (x-axis, y-axis, z-axis). An alternative setup for top-down view (called "2d" mode inside the app) is also provided, which positions the camera directly above the focus point with a rotation of 90 degrees downwards and $\overrightarrow{lc}$ = $(0, z, 0)$. The following figure presents the setup for both cases.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/camera_setup.png}\\
	\caption{Camera setups for "3d" and top-down "2d" view}
\end{figure}

With this setup as a prerequisite, all map-relevant camera transformations can be defined with the parameters $\overrightarrow{p}$ (position of the focus point object), which is responsible for the camera movement, $\varphi$ (rotation of the focus point object around the y-axis), which represents the camera rotation and $z$ (parameter of the local position of the camera), which is the camera's zoom amount.

Besides the transform parameters of the camera, two other important render settings need to be considered for the campus map: On the one hand, there is the projection mode, which determines how the 3d environment is projected onto the 2d screen. Here the perspective projection is chosen since it resembles the real-world projection of the human eye and therefore creates an intuitive feeling of depth inside the scene. On the other hand, the field-of-view (FOV) value, which determines the opening angle of the camera viewport is set to 55 degrees. 

\subsection{Implementing common interaction gestures}
Touch gestures are an established way of controlling the previously defined parameters for movement, rotation and camera zoom. This section explains which gestures are chosen and how they are implemented for this thesis.

One prerequisite for a smooth and robust interaction experience is the use of animation when moving, rotating and zooming on the map. A parameter is therefore not directly set, but rather linearly interpolated over time between its current value and a desired target value. From a programming perspective, the system constantly tries to perform such an interpolation and only stops, when the actual parameter is equal to the target one. Therefore the only parameters that need to be set while interacting with the map are the target values, which are hereafter denoted as $\overrightarrow{p_{t}}$, $\varphi_{t}$ and $z_{t}$.\\

Map movement: The user can move the camera on the campus map by dragging a finger over the touchscreen. The internal evaluation of this process can be broken down into multiple steps:

\begin{enumerate}
	\item The user's finger touches the screen for the first time (touch start): The initial touch position $\overrightarrow{t_{start}}$ is detected.
	\item The user slides the finger over the touchscreen (touch moved): The current touch position $\overrightarrow{t_{cur}}$ is detected and the desired camera position is calculated as $\overrightarrow{p_{t}} = \overrightarrow{p} + (\overrightarrow{t_{start}} - \overrightarrow{t_{cur}}) * \alpha$, where $\alpha$ determines the drag speed of the campus map and is chosen as 5.
\end{enumerate}

Pinch-to-zoom: Pinching is a type of gesture where the user positions two fingers on the touchscreen and an action is triggered when the distance between both touchpoints changes. In this case, the camera should zoom in when the user brings the fingers closer together and vice versa. The following steps are necessary to implement this functionality:

\begin{enumerate}
	\item Two fingers touch the screen simultaneously for the first time (touch start): Both touch start positions are retrieved and their distance $d_{start}$ is calculated.
	\item The user moves one or more fingers (touch move): The current distance $d_{cur}$ between both fingers is calculated. The target zoom is set as $z_{t} = z_{t} + (d_{cur} - d_{start}) * s$, where $s$ is the strength of the zoom and is selected as 0,1.
\end{enumerate}

Rotation of the map: The gesture for map rotation is similar to the pinch gesture from the zoom functionality, but rather takes the rotation angle between the start- and end position of both fingers into account, instead of their distance. This can be also realized in two different steps:

\begin{enumerate}
	\item Two fingers touch the screen simultaneously for the first time (touch start): The difference between both touch positions is stored as a 2-dimensional vector $\overrightarrow{t_{sdiff}}$.
	\item  The user moves one or more fingers (touch move): The current difference of touch positions is also stored as a 2-dimensional vector $\overrightarrow{t_{cdiff}}$. The camera rotation is updated with $\varphi_{t} = \varphi + singedAngle(\overrightarrow{t_{sdiff}}, \overrightarrow{t_{cdiff}}) * \beta$, where $singleAngle(a, b)$ is a function that returns the signed angle between two 2-dimensional vectors and $\beta$ is a parameter for the rotational strength and practically set to 0,5.
\end{enumerate}

\subsection{Map design}
The last step in the campus map generation process is the elaboration of a conceptually fitting design. The requirements for such a design mainly consist of the fact that all important landmarks should be easily identifiable and explorable by the user as well as that the map should fit into the already created UI design.

For the color scheme of the map, inspiration can be taken from already established map services \cite{google_maps_website} \cite{apple_maps_website} and the real-world counterparts of the presented entities. Green areas are displayed in a green tone (HEX \#71C79D), water is displayed in blue (HEX \#90CEFF), the street network is white (HEX \#FFFFFF) and buildings are presented in a neutral grey tone (HEX \#E6E6E6).

In Unity3d, colors are applied through so-called material objects, which further also contain parameters for lighting calculations. One material is chosen per entity. The only rendering parameters, that are modified for every material, are the ones that define the light calculation model as well as shadow casting options. The latter one is turned on, which results in the fact that in contrast to \cite{google_maps_website} and \cite{apple_maps_website}, all entities on the map cast and receive shadows. The light rendering mode, on the other hand, is set to the metallic workflow mode, which offers specular reflections based on predefined "metallic" and "smoothness" values. These values are set individually for every material type.

To emphasize the 3d environment as well as to form additional reference points for orientation, tree models are placed in the green areas on the north- and south sections of the campus. Furthermore, additional building structures are manually applied onto certain buildings (e.g., Hauptgebäude and Mathematikgebäude), to more closely resemble their respective real-world shapes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/generated_campus_map.png}\\
	\caption{Overview of the generated campus map}
\end{figure}

\section{Navigation system development}
Test.

\subsection{Representation of geodata for navigation}
Test.

\subsection{Manual generation of TU Berlin's street network}
\subsection{Routing across the campus}
\subsection{Time estimation for routes}
\subsection{Embedding the current user location via GPS}

\section{Interactive information layer development}
\subsection{Collection of campus relevant information from the web}
\subsection{Processing of information and internal representation}

\section{User interface development}
\subsection{Navigation system}
\subsection{Information layer}
\subsection{Enhancing the user experience with additional screens and features}